import os
import requests
import time
import json
import multiprocessing

# Your API Keys here
API_KEYS = [
    "KEY1",
    "KEY2",
    "KEY3",
    "KEY4"
]

DIRECTORY = "src/data/pyinstaller_3x"
REPORTS_DIRECTORY = "reports"
WAIT_TIME = 30  # Time to wait before fetching the report, adjust as needed
MAX_RETRIES = 5  # Maximum number of retries if a report is queued
CHECK_UNPROCESSED = True  # Flag to check unprocessed files after termination


# Upload file to VirusTotal
def upload_file(file_path, api_key):
    try:
        url = "https://www.virustotal.com/vtapi/v2/file/scan"
        with open(file_path, "rb") as file:
            files = {"file": (file_path, file)}
            params = {"apikey": api_key}
            response = requests.post(url, files=files, params=params)
        return response.json()
    except Exception as e:
        print(f"Error uploading file {file_path}: {e}")
        return None


# Get report from VirusTotal
def get_report(resource, api_key):
    try:
        url = "https://www.virustotal.com/vtapi/v2/file/report"
        params = {"apikey": api_key, "resource": resource}
        response = requests.get(url, params=params)
        return response.json()
    except Exception as e:
        print(f"Error getting report for {resource}: {e}")
        return None


# Save report as JSON
def save_clean_json(report, output_file):
    json_string = json.dumps(report)
    json_string = json_string.replace("True", "true").replace("False", "false").replace("None", "null")

    # Write the cleaned JSON to a file
    with open(output_file, 'w') as f:
        f.write(json_string)

    print(f"Clean JSON saved to {output_file}")


# Check if the report is still queued on VirusTotal
def is_report_queued(report):
    return report.get("response_code") == -2


# Process files for a specific branch (using one API key)
def process_files(branch_id, api_key, files):
    print(f"Process number {branch_id} started.")

    # Ensure the reports directory exists
    if not os.path.exists(REPORTS_DIRECTORY):
        os.makedirs(REPORTS_DIRECTORY)

    for i, file_name in enumerate(files):
        file_path = os.path.join(DIRECTORY, file_name)
        json_output_name = os.path.join(REPORTS_DIRECTORY, f"report_{os.path.splitext(file_name)[0]}.json")
        print(f"Checking file {file_name} (Process {branch_id}, file {i})")

        # Check if the report already exists in the reports directory
        if os.path.exists(json_output_name):
            print(f"Report already exists for {file_name}, skipping...")
            continue  # Skip if the report exists

        # Step 1: Check if the report already exists on VirusTotal
        report = get_report(file_name, api_key)
        if report and report.get("response_code") == 1:
            # Save the cleaned JSON report to a file
            save_clean_json(report, json_output_name)
            print(f"Report for {file_name} already exists on VirusTotal. Saved to {json_output_name}.")
            continue

        # Step 2: Upload the file for scanning if no report exists
        upload_response = upload_file(file_path, api_key)
        if not upload_response:
            print(f"Failed to upload file {file_name}. Skipping...")
            continue

        print(f"Upload Response for {file_name}: {upload_response} (process number {branch_id})")

        # Step 3: Wait for the file to be scanned and get the scan ID
        scan_id = upload_response.get("scan_id")
        if scan_id:
            retries = 0
            while retries < MAX_RETRIES:
                print(
                    f"Waiting for scan to complete (Attempt {retries + 1}/{MAX_RETRIES}) process number {branch_id}...")
                time.sleep(WAIT_TIME)

                report = get_report(scan_id, api_key)
                if report and not is_report_queued(report):
                    break  # Report is ready, exit the loop

                retries += 1

            # Save the report once it's ready
            if report and not is_report_queued(report):
                save_clean_json(report, json_output_name)
            else:
                print(
                    f"Report for {file_name} is still queued after {MAX_RETRIES} attempts. Skipping process number {branch_id}...")
        else:
            print(f"Failed to retrieve scan ID for {file_name}. Skipping process number {branch_id}...")


# Function to check and process unprocessed files after script execution
def check_unprocessed_files():
    print("Checking for unprocessed files...")
    all_binaries = os.listdir(DIRECTORY)
    all_reports = os.listdir(REPORTS_DIRECTORY)

    # Extract the report file names without extensions for comparison
    report_names = {os.path.splitext(report)[0].replace("report_", "") for report in all_reports}

    unprocessed = []
    for file_name in all_binaries:
        if os.path.splitext(file_name)[0] not in report_names:
            unprocessed.append(file_name)

    if unprocessed:
        print(f"{len(unprocessed)} files are unprocessed. Re-running for unprocessed files.")
        return unprocessed
    else:
        print("All files processed successfully.")
        return []


# Parallel processing to split the files across multiple API keys
def parallel_processing():
    # Get the list of files in the directory
    files = os.listdir(DIRECTORY)
    files.sort()  # Sort files to ensure proper distribution

    # Divide the files evenly among the four branches
    file_batches = [files[i::4] for i in range(4)]

    # Create multiprocessing pool and assign each branch with an API key
    pool = multiprocessing.Pool(processes=4)
    for branch_id, (api_key, batch) in enumerate(zip(API_KEYS, file_batches)):
        pool.apply_async(process_files, args=(branch_id, api_key, batch))

    pool.close()
    pool.join()

    # Check for unprocessed files if the flag is set
    if CHECK_UNPROCESSED:
        unprocessed_files = check_unprocessed_files()
        if unprocessed_files:
            pool = multiprocessing.Pool(processes=4)
            for branch_id, (api_key, batch) in enumerate(zip(API_KEYS, [unprocessed_files[i::4] for i in range(4)])):
                pool.apply_async(process_files, args=(branch_id, api_key, batch))
            pool.close()
            pool.join()


if __name__ == "__main__":
    parallel_processing()
