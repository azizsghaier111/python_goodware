{"imports": ["os", "numpy", "torch", "Dataset", "DataLoader", "Accelerator", "DistributedType", "AdamW", "AutoTokenizer", "AutoModelForSequenceClassification", "prepare", "tensorflow", "MagicMock"], "function_calls": ["train", "eval", "Accelerator", "print", "from_pretrained", "TextDataset", "DataLoader", "from_pretrained", "AdamW", "prepare", "range", "print", "print", "len", "tokenizer", "zero_grad", "to", "to", "to", "model", "backward", "step", "parameters", "train", "test", "squeeze", "squeeze", "tensor"], "strings": ["false", "TOKENIZERS_PARALLELISM", "bert-base-cased", "bert-base-cased", "Evaluating...", "Accuracy:", "max_length", "pt", "input_ids", "attention_mask", "labels", "hi", "fp16", "cpu", "distributed_type", "batch_size", "num_workers", "padding", "truncation", "max_length", "return_tensors", "input_ids", "attention_mask", "labels", "attention_mask", "labels", "input_ids", "attention_mask", "dtype"]}