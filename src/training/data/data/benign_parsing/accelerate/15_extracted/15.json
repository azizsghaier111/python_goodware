{"imports": ["os", "numpy", "torch", "Dataset", "DataLoader", "Accelerator", "DistributedType", "AdamW", "AutoTokenizer", "AutoModelForSequenceClassification", "convert_graph_to_onnx", "accuracy_score"], "function_calls": ["train", "eval", "Accelerator", "from_pretrained", "to", "from_pretrained", "AdamW", "prepare", "mock_texts_labels", "TextDataset", "DataLoader", "range", "print", "convert", "main", "len", "tokenizer", "zero_grad", "to", "to", "to", "model", "backward", "step", "no_grad", "parameters", "print", "train", "print", "evaluate", "print", "flatten", "flatten", "tensor", "to", "to", "to", "model", "extend", "extend", "flatten", "flatten", "argmax", "numpy", "numpy", "cpu", "cpu"], "strings": ["false", "__main__", "TOKENIZERS_PARALLELISM", "distilbert-base-uncased", "distilbert-base-uncased", "Converting to TensorFlow model...", "tf_models/onnx/model.onnx", "tf_model", "max_length", "pt", "input_ids", "attention_mask", "labels", "Evaluating...", "fp16", "cpu", "distributed_type", "num_labels", "lr", "batch_size", "shuffle", "Epoch ", " - Training...", "Accuracy: ", "padding", "truncation", "max_length", "return_tensors", "input_ids", "attention_mask", "labels", "attention_mask", "labels", "input_ids", "attention_mask", "dtype", "input_ids", "attention_mask", "labels", "attention_mask", "labels", "axis"]}