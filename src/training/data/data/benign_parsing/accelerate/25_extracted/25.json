{"imports": ["os", "logging", "numpy", "torch", "DataLoader", "GradScaler", "Accelerator", "DistributedType", "AdamW", "AutoTokenizer", "AutoModelForSequenceClassification", "accuracy_score"], "function_calls": ["basicConfig", "getLogger", "train", "Accelerator", "from_pretrained", "to", "from_pretrained", "AdamW", "GradScaler", "TextDataset", "DataLoader", "range", "save_pretrained", "main", "len", "tokenizer", "parameters", "train", "squeeze", "squeeze", "tensor"], "strings": ["%(asctime)s - %(levelname)s - %(name)s -   %(message)s", "%m/%d/%Y %H:%M:%S", "./model_output", "__main__", "distilbert-base-uncased", "distilbert-base-uncased", "format", "datefmt", "level", "max_length", "pt", "input_ids", "attention_mask", "labels", "", "", "fp16", "cpu", "distributed_type", "num_labels", "batch_size", "padding", "truncation", "max_length", "return_tensors", "input_ids", "attention_mask", "dtype"]}