{"imports": ["os", "logging", "numpy", "torch", "Dataset", "DataLoader", "Accelerator", "DistributedType", "AdamW", "AutoTokenizer", "AutoModelForSequenceClassification", "accuracy_score"], "function_calls": ["basicConfig", "getLogger", "train", "eval", "makedirs", "save_pretrained", "info", "Accelerator", "from_pretrained", "to", "from_pretrained", "AdamW", "prepare", "info", "load_data", "info", "TextDataset", "DataLoader", "range", "info", "save_model", "main", "len", "tokenizer", "zero_grad", "to", "to", "to", "model", "backward", "step", "no_grad", "open", "readlines", "hasattr", "parameters", "info", "train", "info", "evaluate", "info", "flatten", "flatten", "tensor", "to", "to", "to", "model", "extend", "extend", "flatten", "flatten", "argmax", "numpy", "numpy", "cpu", "cpu"], "strings": ["%(asctime)s - %(levelname)s - %(name)s -   %(message)s", "%m/%d/%Y %H:%M:%S", "./model_output", "./dummy_data.txt", "__main__", "distilbert-base-uncased", "distilbert-base-uncased", "Loading data...", "Creating datasets...", "Save trained model...", "format", "datefmt", "level", "max_length", "pt", "input_ids", "attention_mask", "labels", "r", "module", "Model saved in ", "Training...", "Evaluating...", "exist_ok", "fp16", "cpu", "distributed_type", "num_labels", "batch_size", "Accuracy: ", "padding", "truncation", "max_length", "return_tensors", "input_ids", "attention_mask", "labels", "attention_mask", "labels", "input_ids", "attention_mask", "dtype", "input_ids", "attention_mask", "labels", "attention_mask", "labels", "axis"]}