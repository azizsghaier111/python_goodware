{"imports": ["os", "logging", "numpy", "torch", "Dataset", "DataLoader", "Accelerator", "DistributedType", "AutoTokenizer", "AutoModelForSequenceClassification", "AdamW", "accuracy_score"], "function_calls": ["basicConfig", "getLogger", "train", "enumerate", "eval", "Accelerator", "from_pretrained", "to", "prepare", "from_pretrained", "TextDataset", "DataLoader", "AdamW", "range", "main", "len", "tokenizer", "to", "to", "to", "model", "backward", "step", "zero_grad", "no_grad", "enumerate", "parameters", "train", "evaluate", "print", "flatten", "flatten", "tensor", "to", "to", "to", "model", "append", "append", "numpy", "numpy", "cpu", "cpu", "detach", "detach", "argmax"], "strings": ["%(asctime)s - %(levelname)s - %(name)s -   %(message)s", "%m/%d/%Y %H:%M:%S", "__main__", "bert-base-uncased", "bert-base-uncased", "format", "datefmt", "level", "max_length", "pt", "input_ids", "attention_mask", "label", "hello world", "fp16", "cpu", "distributed_type", "num_labels", "batch_size", "Epoch: ", " - Score: ", "truncation", "max_length", "padding", "return_tensors", "input_ids", "attention_mask", "label", "attention_mask", "labels", "input_ids", "attention_mask", "dtype", "input_ids", "attention_mask", "label", "attention_mask", "labels", ".4f"]}