{"imports": ["os", "numpy", "torch", "Dataset", "DataLoader", "Accelerator", "DistributedType", "AdamW", "AutoTokenizer", "AutoModelForSequenceClassification", "accuracy_score"], "function_calls": ["train", "eval", "Accelerator", "from_pretrained", "to", "from_pretrained", "AdamW", "prepare", "TextDataset", "DataLoader", "range", "main", "len", "tokenizer", "zero_grad", "to", "to", "to", "model", "backward", "step", "no_grad", "parameters", "print", "train", "print", "evaluate", "print", "flatten", "flatten", "tensor", "to", "to", "to", "model", "extend", "extend", "flatten", "flatten", "argmax", "numpy", "numpy", "cpu", "cpu"], "strings": ["__main__", "distilbert-base-uncased", "distilbert-base-uncased", "Hello, this is an example sentence", "And this is another one", "max_length", "pt", "input_ids", "attention_mask", "labels", "Training...", "Evaluating...", "fp16", "cpu", "distributed_type", "num_labels", "batch_size", "Accuracy: ", "padding", "truncation", "max_length", "return_tensors", "input_ids", "attention_mask", "labels", "attention_mask", "labels", "input_ids", "attention_mask", "dtype", "input_ids", "attention_mask", "labels", "attention_mask", "labels", "axis"]}