{"imports": ["torch", "GPT2LMHeadModel", "GPT2Tokenizer", "pipeline"], "function_calls": ["from_pretrained", "from_pretrained", "tokenize", "convert_tokens_to_ids", "convert_ids_to_tokens", "tensor", "tensor", "pipeline", "pipeline", "ner_pipe", "from_pretrained", "from_pretrained", "encode", "generate", "decode", "tokenize_text", "print", "convert_token_to_id", "print", "convert_id_to_tokens", "print", "generate_text", "print", "text_completion", "print", "get_sentiment", "print", "get_entities", "print", "simplify_text", "print", "no_grad", "generate", "no_grad", "model", "sentiment_analysis", "encode", "encode", "round"], "strings": ["gpt2", "gpt2", "\n    Function for tokenization\n    ", "\n    Function for converting tokens to ids\n    ", "\n    Function for converting token ids to tokens\n    ", "\n    Function for text generation\n    ", "\n    Function for text completion\n    ", "__main__", "The sky is ", "sentiment-analysis", "label: ", ", with score: ", "ner", "t5-base", "t5-base", "pt", "Tokens from text: ", "Token Ids: ", "Tokens from Ids: ", "Generated text: ", "Completed text: ", "Sentiment: ", "Entities: ", "Simplified text: ", "simplify: ", "label", "grouped_entities", "return_tensors", "max_length", "max_length", "min_length", "length_penalty", "num_beams", "early_stopping", "max_length", "num_return_sequences", "score"]}