{"imports": ["os", "OmegaConf", "DictConfig", "torch", "functional", "pytorch_lightning", "mock", "numpy", "DataLoader", "random_split", "MNIST", "transforms", "Dict", "List", "Any"], "function_calls": ["LitModel", "Trainer", "MNIST", "random_split", "DataLoader", "DataLoader", "fit", "merge", "create", "join", "load_config", "load_config", "merge_configs", "interpolate_config_values", "train", "__init__", "Linear", "relu", "self", "cross_entropy", "self", "cross_entropy", "Adam", "int", "getcwd", "ToTensor", "dirname", "layer", "parameters", "super", "view", "size"], "strings": ["\n    A simple linear model for the MNIST dataset, wrapped in a PyTorch Lightning module.\n\n    Args:\n        config: The configuration for the model.\n    ", "\n    Train the model.\n    ", "\n    Load a configuration file using OmegaConf.\n    ", "\n    Merge two configurations, with the second config taking priority.\n    ", "\n    Interpolate the config values.\n    ", "\n    Load configs, merge them, interpolate the values and start training.\n    ", "\n        Forward pass through the model.\n        ", "\n        The training step for a batch.\n        ", "\n        The validation step for a batch.\n        ", "\n        Configure the optimizer for the model.\n        ", "default.yaml", "max_epochs", "gpus", "download", "transform", "batch_size", "batch_size", "lr"]}