{"imports": ["os", "torch", "OmegaConf", "datasets", "transforms", "functional", "Adam", "TensorBoardLogger", "ModelCheckpoint", "EarlyStopping", "ReduceLROnPlateau", "Trainer", "LightningModule", "DataLoader", "MisconfigurationException", "patch", "ToTensor"], "function_calls": ["MyModel", "train", "MNIST", "int", "random_split", "MyModel", "TensorBoardLogger", "ModelCheckpoint", "EarlyStopping", "LearningRateMonitor", "Trainer", "fit", "main", "__init__", "Linear", "Linear", "Linear", "size", "view", "layer_1", "relu", "layer_2", "relu", "layer_3", "self", "process_batch", "process_batch", "Adam", "Compose", "len", "get", "get", "cross_entropy", "parameters", "len", "super", "ToTensor"], "strings": ["Pytorch Lightning Module inherits from nn.Module\n    At the very least it must define a `forward` and a `training_step`.\n    ", "__main__", "", "/{epoch}_{val_loss:.2f}", "power", "loss", "TB_LOG_DIR", "CHECKPOINTS_DIR", "train", "download", "transform", "save_dir", "filepath", "max_epochs", "gradient_clip_val", "auto_lr_find", "auto_scale_batch_size", "checkpoint_callback", "early_stop_callback", "callbacks", "logger", "lr"]}