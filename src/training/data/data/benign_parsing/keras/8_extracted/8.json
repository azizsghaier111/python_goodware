{"imports": ["numpy", "tensorflow", "pandas", "Adam", "TFBertForSequenceClassification", "BertTokenizer", "train_test_split"], "function_calls": ["numpy", "from_pretrained", "from_pretrained", "CustomModel", "compile", "main", "__init__", "encode_plus", "append", "append", "convert_to_tensor", "convert_to_tensor", "predict", "Adam", "softmax", "super"], "strings": ["\n    Custom model that uses a transformer model for sequence classification\n    ", "\n    Function to preprocess the textual data by encoding and converting the sentences into tensors\n    ", "\n    Function to predict the sentiment of the text using the pre-trained model\n    ", "__main__", "input_ids", "attention_mask", "bert-base-uncased", "bert-base-uncased", "max_length", "tf", "num_labels", "transformer_model", "optimizer", "truncation", "add_special_tokens", "max_length", "padding", "return_attention_mask", "return_tensors", "input_ids", "attention_mask", "lr", "axis"]}