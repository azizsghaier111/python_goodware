{"imports": ["numpy", "tensorflow", "Adam", "TFBertForSequenceClassification", "BertTokenizer", "train_test_split"], "function_calls": ["train_test_split", "fit", "numpy", "from_pretrained", "from_pretrained", "CustomModel", "compile", "print", "prepare_data", "training", "predict", "enumerate", "main", "__init__", "encode_plus", "append", "append", "convert_to_tensor", "convert_to_tensor", "predict", "Adam", "load_weights", "print", "print", "softmax", "super", "max", "argmax"], "strings": ["__main__", "input_ids", "attention_mask", "input_ids", "attention_mask", "bert-base-uncased", "bert-base-uncased", "No model weights found.", "This product is very good!", "This service is terrible.", "max_length", "tf", "model.h5", "Model weights loaded successfully.", "test_size", "random_state", "epochs", "validation_data", "num_labels", "transformer_model", "optimizer", "The sentiment for the text '", "' is ", " with a confidence of ", ".", "truncation", "add_special_tokens", "max_length", "padding", "return_attention_mask", "return_tensors", "input_ids", "attention_mask", "lr", "axis", "positive", "negative"]}