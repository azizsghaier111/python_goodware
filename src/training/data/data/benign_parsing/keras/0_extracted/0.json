{"imports": ["tensorflow", "Adam", "TFBertForSequenceClassification", "BertTokenizer", "numpy"], "function_calls": ["encode_plus", "array", "array", "reshape", "reshape", "numpy", "from_pretrained", "from_pretrained", "CustomModel", "compile", "load_weights", "prepare_data", "predict", "print", "main", "__init__", "transformer_model", "predict", "Adam", "softmax", "super", "argmax"], "strings": ["This product is very good!", "__main__", "tf", "input_ids", "attention_mask", "bert-base-uncased", "bert-base-uncased", "model.h5", "The text is ", "add_special_tokens", "max_length", "pad_to_max_length", "return_attention_mask", "return_tensors", "input_ids", "attention_mask", "transformer_model", "optimizer", "lr", "axis"]}