{"imports": ["copy", "math", "os", "warnings", "List", "Optional", "Tuple", "Union", "torch", "nn", "BCEWithLogitsLoss", "CrossEntropyLoss", "MSELoss", "ACT2FN", "BaseModelOutput", "BaseModelOutputWithPastAndCrossAttentions", "Seq2SeqLMOutput", "Seq2SeqModelOutput", "Seq2SeqQuestionAnsweringModelOutput", "Seq2SeqSequenceClassifierOutput", "TokenClassifierOutput", "PreTrainedModel", "ALL_LAYERNORM_LAYERS", "find_pruneable_heads_and_indices", "prune_linear_layer", "DUMMY_INPUTS", "DUMMY_MASK", "add_start_docstrings", "add_start_docstrings_to_model_forward", "is_torch_fx_proxy", "logging", "replace_return_docstrings", "assert_device_map", "get_device_map", "T5Config", "T5_PRETRAINED_MODEL_ARCHIVE_LIST"], "function_calls": ["get_logger", "__init__", "Parameter", "mean", "ones", "rsqrt", "to", "super", "pow", "to"], "strings": [" PyTorch T5 model.", "T5Config", "google-t5/t5-small", "\n    This is an experimental feature and is a subject to change at a moment's notice.\n\n    Uses a device map to distribute attention modules of the model across several devices. If no device map is given,\n    it will evenly distribute blocks across all devices.\n\n    Args:\n        device_map (`Dict[int, list]`, optional, defaults to None):\n            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n            have fewer attention modules mapped to it than other devices. For reference, the t5 models have the\n            following number of attention modules:\n\n                - google-t5/t5-small: 6\n                - google-t5/t5-base: 12\n                - google-t5/t5-large: 24\n                - google-t5/t5-3b: 24\n                - google-t5/t5-11b: 24\n\n    Example:\n\n    ```python\n    # Here is an example of a device map on a machine with 4 GPUs using google-t5/t5-3b, which has a total of 24 attention modules:\n    model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-3b\")\n    device_map = {\n        0: [0, 1, 2],\n        1: [3, 4, 5, 6, 7, 8, 9],\n        2: [10, 11, 12, 13, 14, 15, 16],\n        3: [17, 18, 19, 20, 21, 22, 23],\n    }\n    model.parallelize(device_map)\n    ```\n", "\n    Moves the model to cpu from a model parallel state.\n\n    Example:\n\n    ```python\n    # On a 4 GPU machine with google-t5/t5-3b:\n    model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-3b\")\n    device_map = {\n        0: [0, 1, 2],\n        1: [3, 4, 5, 6, 7, 8, 9],\n        2: [10, 11, 12, 13, 14, 15, 16],\n        3: [17, 18, 19, 20, 21, 22, 23],\n    }\n    model.parallelize(device_map)  # Splits the model across several devices\n    model.deparallelize()  # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n    ```\n", "Load tf checkpoints in a pytorch model.", "\n        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.\n        ", "keepdim"]}