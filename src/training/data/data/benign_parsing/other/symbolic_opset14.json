{"imports": ["annotations", "functools", "Optional", "torch", "_constants", "_type_utils", "symbolic_helper", "GLOBALS", "_beartype", "jit_utils", "registration"], "function_calls": ["partial", "_onnx_symbolic", "parse_args", "op", "_onnx_symbolic", "op", "_onnx_symbolic", "op", "quantized_args", "parse_args", "_reshape_helper", "_onnx_symbolic", "parse_args", "check_training_mode", "_batchnorm_helper", "op", "setType", "setType", "_onnx_symbolic", "dequantize_helper", "hardswish", "quantize_helper", "_onnx_symbolic", "parse_args", "op", "op", "op", "op", "op", "op", "op", "op", "op", "op", "op", "op", "op", "op", "op", "op", "op", "op", "op", "is_autocast_enabled", "args_have_same_dtype", "_onnx_opset_unsupported_detailed", "type", "type", "op", "op", "onnx_type", "tensor", "op", "onnx_type", "tensor", "tensor", "tensor", "tensor", "tensor", "op", "tensor", "tensor", "_onnx_symbolic", "from_value", "from_value", "float"], "strings": ["This file exports ONNX ops for opset 14.\n\nNote [ONNX operators that are added/updated in opset 14]\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nNew operators:\n    HardSwish, Trilu\n\nUpdated operators:\n    Reshape\n    Add, Sub, Mul, Div\n    GRU, LSTM, RNN\n    BatchNorm, Cumsum, Relu\n", "hardswish", "tril", "triu", "reshape", "batch_norm", "quantized_hardswish", "scaled_dot_product_attention", "Calculate the scale factor for the attention result.\n\n    Args:\n        query: Tensor of shape [..., L, E]\n\n    Returns:\n        Scalar scale factor := 1 / math.sqrt(query.size(-1))\n    ", "Create a causal mask for the given query and key tensors.\n\n    Equivalent to::\n        mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n        attn_mask = torch.zeros(L, S, dtype=torch.float)\n        attn_mask = attn_mask.masked_fill(not mask, -float('inf'))\n\n    Args:\n        query: Tensor of shape [..., L, E]\n        key: Tensor of shape [..., S, E]\n\n    Returns:\n        Tensor of shape [L, S]\n    ", "aten::hardswish", "v", "HardSwish", "aten::tril", "Trilu", "aten::triu", "Trilu", "v", "v", "aten::batch_norm", "v", "v", "v", "v", "v", "i", "f", "f", "i", "g", "jit_utils.GraphContext", "batch_norm", "BatchNormalization", "quantized::hardswish", "g", "jit_utils.GraphContext", "aten::scaled_dot_product_attention", "v", "v", "v", "v", "f", "b", "v", "g", "jit_utils.GraphContext", "query", "torch._C.Value", "key", "torch._C.Value", "value", "torch._C.Value", "attn_mask", "Optional[torch._C.Value]", "dropout_p", "float", "is_causal", "bool", "scale", "Optional[torch._C.Value]", "Shape", "Slice", "Cast", "Constant", "Div", "Cast", "Shape", "Shape", "Constant", "Constant", "Slice", "Slice", "Concat", "Constant", "Expand", "Trilu", "Constant", "Constant", "Where", "opset", "BatchNormalization", "All input tensors must have the same `dtype`. Turn off Autocast or export using opset version 15.", "Constant", "Constant", "Sqrt", "Equal", "upper_i", "upper_i", "allowzero", "epsilon_f", "momentum_f", "training_mode_i", "outputs", "to_i", "value_t", "to_i", "value_t", "value_t", "axis_i", "value_t", "upper_i", "value_t", "value_t", "aten::reshape", "value_t", "value_t", "dtype", "dtype", "dtype", "dtype", "dtype", "inf"]}