{"imports": ["gc", "logging", "Any", "Dict", "Optional", "Union", "gymnasium", "Algorithm", "AlgorithmConfig", "NotProvided", "DreamerV3Catalog", "do_symlog_obs", "DreamerV3EnvRunner", "report_dreamed_eval_trajectory_vs_samples", "report_predicted_vs_sampled_obs", "report_sampling_and_replay_buffer", "DEFAULT_MODULE_ID", "Columns", "SingleAgentRLModuleSpec", "synchronous_parallel_sample", "SampleBatch", "deep_update", "override", "PublicAPI", "try_import_tf", "one_hot", "ENV_RUNNER_RESULTS", "GARBAGE_COLLECTION_TIMER", "LEARN_ON_BATCH_TIMER", "LEARNER_RESULTS", "NUM_AGENT_STEPS_SAMPLED", "NUM_AGENT_STEPS_SAMPLED_LIFETIME", "NUM_ENV_STEPS_SAMPLED", "NUM_ENV_STEPS_SAMPLED_LIFETIME", "NUM_ENV_STEPS_TRAINED_LIFETIME", "NUM_EPISODES", "NUM_EPISODES_LIFETIME", "NUM_GRAD_UPDATES_LIFETIME", "NUM_SYNCH_WORKER_WEIGHTS", "SAMPLE_TIMER", "SYNCH_WORKER_WEIGHTS_TIMER", "TIMERS", "EpisodeReplayBuffer", "LearningRateOrSchedule", "ResultDict", "ray.rllib.algorithms.dreamerv3.tf.dreamerv3_tf_learner", "ray.rllib.algorithms.dreamerv3.tf.dreamerv3_tf_rl_module"], "function_calls": ["getLogger", "try_import_tf", "property", "__init__", "validate", "is_multi_agent", "override", "NotImplementedError", "override", "DreamerV3Config", "setup", "local_worker", "__setstate__", "int", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "get", "ValueError", "SingleAgentRLModuleSpec", "info", "peek", "peek", "super", "super", "super", "super", "super"], "strings": ["\n[1] Mastering Diverse Domains through World Models - 2023\nD. Hafner, J. Pasukonis, J. Ba, T. Lillicrap\nhttps://arxiv.org/pdf/2301.04104v1.pdf\n\n[2] Mastering Atari with Discrete World Models - 2021\nD. Hafner, T. Lillicrap, M. Norouzi, J. Ba\nhttps://arxiv.org/pdf/2010.02193.pdf\n", "Defines a configuration class from which a DreamerV3 can be built.\n\n    .. testcode::\n\n        from ray.rllib.algorithms.dreamerv3 import DreamerV3Config\n        config = (\n            DreamerV3Config()\n            .environment(\"CartPole-v1\")\n            .training(\n                model_size=\"XS\",\n                training_ratio=1,\n                # TODO\n                model={\n                    \"batch_size_B\": 1,\n                    \"batch_length_T\": 1,\n                    \"horizon_H\": 1,\n                    \"gamma\": 0.997,\n                    \"model_size\": \"XS\",\n                },\n            )\n        )\n\n        config = config.learners(num_learners=0)\n        # Build a Algorithm object from the config and run 1 training iteration.\n        algo = config.build()\n        # algo.train()\n        del algo\n\n    .. testoutput::\n        :hide:\n\n        ...\n    ", "Implementation of the model-based DreamerV3 RL algorithm described in [1].", "Initializes a DreamerV3Config instance.", "XS", "auto", "tf2", "Returns the batch_size_B per Learner worker.\n\n        Needed by some of the DreamerV3 loss math.", "Sets the training related configuration.\n\n        Args:\n            model_size: The main switch for adjusting the overall model size. See [1]\n                (table B) for more information on the effects of this setting on the\n                model architecture.\n                Supported values are \"XS\", \"S\", \"M\", \"L\", \"XL\" (as per the paper), as\n                well as, \"nano\", \"micro\", \"mini\", and \"XXS\" (for RLlib's\n                implementation). See ray.rllib.algorithms.dreamerv3.utils.\n                __init__.py for the details on what exactly each size does to the layer\n                sizes, number of layers, etc..\n            training_ratio: The ratio of total steps trained (sum of the sizes of all\n                batches ever sampled from the replay buffer) over the total env steps\n                taken (in the actual environment, not the dreamed one). For example,\n                if the training_ratio is 1024 and the batch size is 1024, we would take\n                1 env step for every training update: 1024 / 1. If the training ratio\n                is 512 and the batch size is 1024, we would take 2 env steps and then\n                perform a single training update (on a 1024 batch): 1024 / 2.\n            gc_frequency_train_steps: The frequency (in training iterations) with which\n                we perform a `gc.collect()` calls at the end of a `training_step`\n                iteration. Doing this more often adds a (albeit very small) performance\n                overhead, but prevents memory leaks from becoming harmful.\n                TODO (sven): This might not be necessary anymore, but needs to be\n                 confirmed experimentally.\n            batch_size_B: The batch size (B) interpreted as number of rows (each of\n                length `batch_length_T`) to sample from the replay buffer in each\n                iteration.\n            batch_length_T: The batch length (T) interpreted as the length of each row\n                sampled from the replay buffer in each iteration. Note that\n                `batch_size_B` rows will be sampled in each iteration. Rows normally\n                contain consecutive data (consecutive timesteps from the same episode),\n                but there might be episode boundaries in a row as well.\n            horizon_H: The horizon (in timesteps) used to create dreamed data from the\n                world model, which in turn is used to train/update both actor- and\n                critic networks.\n            gae_lambda: The lambda parameter used for computing the GAE-style\n                value targets for the actor- and critic losses.\n            entropy_scale: The factor with which to multiply the entropy loss term\n                inside the actor loss.\n            return_normalization_decay: The decay value to use when computing the\n                running EMA values for return normalization (used in the actor loss).\n            train_critic: Whether to train the critic network. If False, `train_actor`\n                must also be False (cannot train actor w/o training the critic).\n            train_actor: Whether to train the actor network. If True, `train_critic`\n                must also be True (cannot train actor w/o training the critic).\n            intrinsic_rewards_scale: The factor to multiply intrinsic rewards with\n                before adding them to the extrinsic (environment) rewards.\n            world_model_lr: The learning rate or schedule for the world model optimizer.\n            actor_lr: The learning rate or schedule for the actor optimizer.\n            critic_lr: The learning rate or schedule for the critic optimizer.\n            world_model_grad_clip_by_global_norm: World model grad clipping value\n                (by global norm).\n            critic_grad_clip_by_global_norm: Critic grad clipping value\n                (by global norm).\n            actor_grad_clip_by_global_norm: Actor grad clipping value (by global norm).\n            symlog_obs: Whether to symlog observations or not. If set to \"auto\"\n                (default), will check for the environment's observation space and then\n                only symlog if not an image space.\n            use_float16: Whether to train with mixed float16 precision. In this mode,\n                model parameters are stored as float32, but all computations are\n                performed in float16 space (except for losses and distribution params\n                and outputs).\n            replay_buffer_config: Replay buffer config.\n                Only serves in DreamerV3 to set the capacity of the replay buffer.\n                Note though that in the paper ([1]) a size of 1M is used for all\n                benchmarks and there doesn't seem to be a good reason to change this\n                parameter.\n                Examples:\n                {\n                \"type\": \"EpisodeReplayBuffer\",\n                \"capacity\": 100000,\n                }\n\n        Returns:\n            This updated AlgorithmConfig object.\n        ", "Sets the reporting related configuration.\n\n        Args:\n            report_individual_batch_item_stats: Whether to include loss and other stats\n                per individual timestep inside the training batch in the result dict\n                returned by `training_step()`. If True, besides the `CRITIC_L_total`,\n                the individual critic loss values per batch row and time axis step\n                in the train batch (CRITIC_L_total_B_T) will also be part of the\n                results.\n            report_dream_data:  Whether to include the dreamed trajectory data in the\n                result dict returned by `training_step()`. If True, however, will\n                slice each reported item in the dream data down to the shape.\n                (H, B, t=0, ...), where H is the horizon and B is the batch size. The\n                original time axis will only be represented by the first timestep\n                to not make this data too large to handle.\n            report_images_and_videos: Whether to include any image/video data in the\n                result dict returned by `training_step()`.\n            **kwargs:\n\n        Returns:\n            This updated AlgorithmConfig object.\n        ", "Returns the actual training ratio of this Algorithm (not the configured one).\n\n        The training ratio is copmuted by dividing the total number of steps\n        trained thus far (replayed from the buffer) over the total number of actual\n        env steps taken thus far.\n        ", "Sts the algorithm to the provided state\n\n        Args:\n            state: The state dictionary to restore this `DreamerV3` instance to.\n                `state` may have been returned by a call to an `Algorithm`'s\n                `__getstate__()` method.\n        ", "type", "capacity", "EpisodeReplayBuffer", "EpisodeReplayBuffer", "tf2", "tf2", "DreamerV3 does not support the `compute_single_action()` API. Refer to the README here (https://github.com/ray-project/ray/tree/master/rllib/algorithms/dreamerv3) to find more information on how to run action inference with this algorithm.", "`DreamerV3Config.curiosity` is not fully supported and tested yet! It thus remains disabled for now.", "DreamerV3 does NOT support multi-agent setups yet!", "DreamerV3 must be run with `config.api_stack(enable_rl_module_and_learner=True)`!", "Cannot train actor network (`train_actor=True`) w/o training critic! Make sure you either set `train_critic=True` or `train_actor=False`.", "`train_batch_size` should NOT be set! Use `batch_size_B` and `batch_length_T` instead.", "type", "DreamerV3 must be run with the `EpisodeReplayBuffer` type! None other supported.", "The framework ", " is not supported.", "The framework ", " is not supported.", "gamma", "horizon_H", "model_size", "symlog_obs", "use_float16", "batch_length_T", "algo_class", "Your `batch_size_B` (", ") must be a multiple of `num_learners` (", ") in order for DreamerV3 to be able to split batches evenly across your Learner processes.", "Filling replay buffer so it contains at least ", " timesteps (required for a single train batch).", "state", "module_class", "catalog_class", "default", "default"]}