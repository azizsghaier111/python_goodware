{"imports": ["partial", "Any", "List", "Optional", "Union", "torch", "nn", "Tensor", "shufflenetv2", "ImageClassification", "register_model", "Weights", "WeightsEnum", "_IMAGENET_CATEGORIES", "_ovewrite_named_param", "handle_legacy_interface", "ShuffleNet_V2_X0_5_Weights", "ShuffleNet_V2_X1_0_Weights", "ShuffleNet_V2_X1_5_Weights", "ShuffleNet_V2_X2_0_Weights", "_fuse_modules", "_replace_relu", "quantize_model"], "function_calls": ["pop", "channel_shuffle", "quant", "_forward_impl", "dequant", "items", "modules", "_ovewrite_named_param", "verify", "verify", "verify", "verify", "chunk", "cat", "cat", "len", "_ovewrite_named_param", "_fuse_modules", "type", "_fuse_modules", "branch2", "branch1", "branch2", "len", "_fuse_modules", "items"], "strings": ["QuantizableShuffleNetV2", "ShuffleNet_V2_X0_5_QuantizedWeights", "ShuffleNet_V2_X1_0_QuantizedWeights", "ShuffleNet_V2_X1_5_QuantizedWeights", "ShuffleNet_V2_X2_0_QuantizedWeights", "shufflenet_v2_x0_5", "shufflenet_v2_x1_0", "shufflenet_v2_x1_5", "shufflenet_v2_x2_0", "min_size", "categories", "backend", "recipe", "_docs", "fbgemm", "https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models", "\n        These weights were produced by doing Post Training Quantization (eager mode) on top of the unquantized\n        weights listed below.\n    ", "ShuffleNet_V2_X0_5_QuantizedWeights", "ShuffleNet_V2_X1_0_QuantizedWeights", "ShuffleNet_V2_X1_5_QuantizedWeights", "ShuffleNet_V2_X2_0_QuantizedWeights", "\n    Constructs a ShuffleNetV2 with 0.5x output channels, as described in\n    `ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\n    <https://arxiv.org/abs/1807.11164>`__.\n\n    .. note::\n        Note that ``quantize = True`` returns a quantized model with 8 bit\n        weights. Quantized models only support inference and run on CPUs.\n        GPU inference is not yet supported.\n\n    Args:\n        weights (:class:`~torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights` or :class:`~torchvision.models.ShuffleNet_V2_X0_5_Weights`, optional): The\n            pretrained weights for the model. See\n            :class:`~torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the download to stderr.\n            Default is True.\n        quantize (bool, optional): If True, return a quantized version of the model.\n            Default is False.\n        **kwargs: parameters passed to the ``torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/shufflenetv2.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights\n        :members:\n\n    .. autoclass:: torchvision.models.ShuffleNet_V2_X0_5_Weights\n        :members:\n        :noindex:\n    ", "\n    Constructs a ShuffleNetV2 with 1.0x output channels, as described in\n    `ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\n    <https://arxiv.org/abs/1807.11164>`__.\n\n    .. note::\n        Note that ``quantize = True`` returns a quantized model with 8 bit\n        weights. Quantized models only support inference and run on CPUs.\n        GPU inference is not yet supported.\n\n    Args:\n        weights (:class:`~torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights` or :class:`~torchvision.models.ShuffleNet_V2_X1_0_Weights`, optional): The\n            pretrained weights for the model. See\n            :class:`~torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the download to stderr.\n            Default is True.\n        quantize (bool, optional): If True, return a quantized version of the model.\n            Default is False.\n        **kwargs: parameters passed to the ``torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/shufflenetv2.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights\n        :members:\n\n    .. autoclass:: torchvision.models.ShuffleNet_V2_X1_0_Weights\n        :members:\n        :noindex:\n    ", "\n    Constructs a ShuffleNetV2 with 1.5x output channels, as described in\n    `ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\n    <https://arxiv.org/abs/1807.11164>`__.\n\n    .. note::\n        Note that ``quantize = True`` returns a quantized model with 8 bit\n        weights. Quantized models only support inference and run on CPUs.\n        GPU inference is not yet supported.\n\n    Args:\n        weights (:class:`~torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights` or :class:`~torchvision.models.ShuffleNet_V2_X1_5_Weights`, optional): The\n            pretrained weights for the model. See\n            :class:`~torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the download to stderr.\n            Default is True.\n        quantize (bool, optional): If True, return a quantized version of the model.\n            Default is False.\n        **kwargs: parameters passed to the ``torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/shufflenetv2.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights\n        :members:\n\n    .. autoclass:: torchvision.models.ShuffleNet_V2_X1_5_Weights\n        :members:\n        :noindex:\n    ", "\n    Constructs a ShuffleNetV2 with 2.0x output channels, as described in\n    `ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\n    <https://arxiv.org/abs/1807.11164>`__.\n\n    .. note::\n        Note that ``quantize = True`` returns a quantized model with 8 bit\n        weights. Quantized models only support inference and run on CPUs.\n        GPU inference is not yet supported.\n\n    Args:\n        weights (:class:`~torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights` or :class:`~torchvision.models.ShuffleNet_V2_X2_0_Weights`, optional): The\n            pretrained weights for the model. See\n            :class:`~torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the download to stderr.\n            Default is True.\n        quantize (bool, optional): If True, return a quantized version of the model.\n            Default is False.\n        **kwargs: parameters passed to the ``torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/shufflenetv2.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights\n        :members:\n\n    .. autoclass:: torchvision.models.ShuffleNet_V2_X2_0_Weights\n        :members:\n        :noindex:\n    ", "Fuse conv/bn/relu modules in shufflenetv2 model\n\n        Fuse conv+bn+relu/ conv+relu/conv+bn modules to prepare for quantization.\n        Model is modified in place.\n\n        .. note::\n            Note that this operation does not change numerics\n            and the model after modification is in floating point\n        ", "backend", "fbgemm", "num_classes", "backend", "backend", "categories", "backend", "dim", "dim", "dim", "conv1", "conv5", "0", "1", "2", "inplace", "0", "1", "2", "3", "4", "5", "6", "7", "inplace", "0", "1", "2", "3", "4", "inplace"]}