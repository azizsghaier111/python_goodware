{"imports": ["Mapping", "PretrainedConfig", "OnnxSeq2SeqConfigWithPast", "logging", "LONGT5_PRETRAINED_CONFIG_ARCHIVE_MAP"], "function_calls": ["get_logger", "split", "ValueError", "fill_with_past_key_values_", "len", "len"], "strings": [" LongT5 model configuration", "\n    This is the configuration class to store the configuration of a [`LongT5Model`] or a [`FlaxLongT5Model`]. It is\n    used to instantiate a LongT5 model according to the specified arguments, defining the model architecture.\n    Instantiating a configuration with the defaults will yield a similar configuration to that of the LongT5\n    [google/long-t5-local-base](https://huggingface.co/google/long-t5-local-base) architecture.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n    Arguments:\n        vocab_size (`int`, *optional*, defaults to 32128):\n            Vocabulary size of the LongT5 model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`LongT5Model`].\n        d_model (`int`, *optional*, defaults to 512):\n            Size of the encoder layers and the pooler layer.\n        d_kv (`int`, *optional*, defaults to 64):\n            Size of the key, query, value projections per attention head. `d_kv` has to be equal to `d_model //\n            num_heads`.\n        d_ff (`int`, *optional*, defaults to 2048):\n            Size of the intermediate feed forward layer in each `LongT5Block`.\n        num_layers (`int`, *optional*, defaults to 6):\n            Number of hidden layers in the Transformer encoder.\n        num_decoder_layers (`int`, *optional*):\n            Number of hidden layers in the Transformer decoder. Will use the same value as `num_layers` if not set.\n        num_heads (`int`, *optional*, defaults to 8):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        local_radius (`int`, *optional*, defaults to 127)\n            Number of tokens to the left/right for each token to locally self-attend in a local attention mechanism.\n        global_block_size (`int`, *optional*, defaults to 16)\n            Lenght of blocks an input sequence is divided into for a global token representation. Used only for\n            `encoder_attention_type = \"transient-global\"`.\n        relative_attention_num_buckets (`int`, *optional*, defaults to 32):\n            The number of buckets to use for each attention layer.\n        relative_attention_max_distance (`int`, *optional*, defaults to 128):\n            The maximum distance of the longer sequences for the bucket separation.\n        dropout_rate (`float`, *optional*, defaults to 0.1):\n            The ratio for all dropout layers.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-6):\n            The epsilon used by the layer normalization layers.\n        initializer_factor (`float`, *optional*, defaults to 1):\n            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n            testing).\n        feed_forward_proj (`string`, *optional*, defaults to `\"relu\"`):\n            Type of feed forward layer to be used. Should be one of `\"relu\"` or `\"gated-gelu\"`. LongT5v1.1 uses the\n            `\"gated-gelu\"` feed forward projection. Original LongT5 implementation uses `\"gated-gelu\"`.\n        encoder_attention_type (`string`, *optional*, defaults to `\"local\"`):\n            Type of encoder attention to be used. Should be one of `\"local\"` or `\"transient-global\"`, which are\n            supported by LongT5 implementation.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models).\n    ", "longt5", "past_key_values", "hidden_size", "num_attention_heads", "num_hidden_layers", "d_model", "num_heads", "num_layers", "-", "gated", "gated-gelu", "gelu_new", "input_ids", "attention_mask", "past_encoder_sequence + sequence", "relu", "local", "gated", "batch", "encoder_sequence", "batch", "encoder_sequence", "decoder_input_ids", "batch", "decoder_attention_mask", "batch", "past_decoder_sequence + sequence", "decoder_input_ids", "batch", "decoder_sequence", "decoder_attention_mask", "batch", "decoder_sequence", "inputs", "`feed_forward_proj`: ", " is not a valid activation function of the dense layer. Please make sure `feed_forward_proj` is of the format `gated-{ACT_FN}` or `{ACT_FN}`, e.g. 'gated-gelu' or 'relu'", "attention_mask", "direction"]}