{"imports": ["Mapping", "PretrainedConfig", "OnnxSeq2SeqConfigWithPast", "logging"], "function_calls": ["get_logger", "split", "ValueError", "fill_with_past_key_values_", "len", "len"], "strings": [" mT5 model configuration", "\n    This is the configuration class to store the configuration of a [`MT5Model`] or a [`TFMT5Model`]. It is used to\n    instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the mT5\n    [google/mt5-small](https://huggingface.co/google/mt5-small) architecture.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n    Arguments:\n        vocab_size (`int`, *optional*, defaults to 250112):\n            Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`T5Model`] or [`TFT5Model`].\n        d_model (`int`, *optional*, defaults to 512):\n            Size of the encoder layers and the pooler layer.\n        d_kv (`int`, *optional*, defaults to 64):\n            Size of the key, query, value projections per attention head. In the conventional context, it is typically expected that `d_kv` has to be equal to `d_model // num_heads`.\n            But in the architecture of mt5-small, `d_kv` is not equal to `d_model //num_heads`. The `inner_dim` of the projection layer will be defined as `num_heads * d_kv`.\n        d_ff (`int`, *optional*, defaults to 1024):\n            Size of the intermediate feed forward layer in each `T5Block`.\n        num_layers (`int`, *optional*, defaults to 8):\n            Number of hidden layers in the Transformer encoder.\n        num_decoder_layers (`int`, *optional*):\n            Number of hidden layers in the Transformer decoder. Will use the same value as `num_layers` if not set.\n        num_heads (`int`, *optional*, defaults to 6):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        relative_attention_num_buckets (`int`, *optional*, defaults to 32):\n            The number of buckets to use for each attention layer.\n        relative_attention_max_distance (`int`, *optional*, defaults to 128):\n            The maximum distance of the longer sequences for the bucket separation.\n        dropout_rate (`float`, *optional*, defaults to 0.1):\n            The ratio for all dropout layers.\n        classifier_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for classifier.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-6):\n            The epsilon used by the layer normalization layers.\n        initializer_factor (`float`, *optional*, defaults to 1):\n            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n            testing).\n        feed_forward_proj (`string`, *optional*, defaults to `\"gated-gelu\"`):\n            Type of feed forward layer to be used. Should be one of `\"relu\"` or `\"gated-gelu\"`.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models).\n    ", "mt5", "past_key_values", "hidden_size", "num_attention_heads", "num_hidden_layers", "d_model", "num_heads", "num_layers", "-", "gated", "gated-gelu", "gelu_new", "input_ids", "attention_mask", "past_encoder_sequence + sequence", "gated-gelu", "T5Tokenizer", "gated", "batch", "encoder_sequence", "batch", "encoder_sequence", "decoder_input_ids", "batch", "decoder_attention_mask", "batch", "past_decoder_sequence + sequence", "decoder_input_ids", "batch", "decoder_sequence", "decoder_attention_mask", "batch", "decoder_sequence", "inputs", "`feed_forward_proj`: ", " is not a valid activation function of the dense layer. Please make sure `feed_forward_proj` is of the format `gated-{ACT_FN}` or `{ACT_FN}`, e.g. 'gated-gelu' or 'relu'", "attention_mask", "direction"]}