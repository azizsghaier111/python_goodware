{"imports": ["partial", "Any", "Optional", "Union", "nn", "Tensor", "DeQuantStub", "QuantStub", "InvertedResidual", "MobileNet_V2_Weights", "MobileNetV2", "Conv2dNormActivation", "ImageClassification", "register_model", "Weights", "WeightsEnum", "_IMAGENET_CATEGORIES", "_ovewrite_named_param", "handle_legacy_interface", "_fuse_modules", "_replace_relu", "quantize_model"], "function_calls": ["Weights", "pop", "conv", "range", "quant", "_forward_impl", "dequant", "modules", "partial", "verify", "_ovewrite_named_param", "add", "len", "len", "_ovewrite_named_param", "conv", "type", "_fuse_modules", "type", "_fuse_modules", "type", "fuse_model", "str", "str"], "strings": ["QuantizableMobileNetV2", "MobileNet_V2_QuantizedWeights", "mobilenet_v2", "\n    Constructs a MobileNetV2 architecture from\n    `MobileNetV2: Inverted Residuals and Linear Bottlenecks\n    <https://arxiv.org/abs/1801.04381>`_.\n\n    .. note::\n        Note that ``quantize = True`` returns a quantized model with 8 bit\n        weights. Quantized models only support inference and run on CPUs.\n        GPU inference is not yet supported.\n\n    Args:\n        weights (:class:`~torchvision.models.quantization.MobileNet_V2_QuantizedWeights` or :class:`~torchvision.models.MobileNet_V2_Weights`, optional): The\n            pretrained weights for the model. See\n            :class:`~torchvision.models.quantization.MobileNet_V2_QuantizedWeights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True.\n        quantize (bool, optional): If True, returns a quantized version of the model. Default is False.\n        **kwargs: parameters passed to the ``torchvision.models.quantization.QuantizableMobileNetV2``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/mobilenetv2.py>`_\n            for more details about this class.\n    .. autoclass:: torchvision.models.quantization.MobileNet_V2_QuantizedWeights\n        :members:\n    .. autoclass:: torchvision.models.MobileNet_V2_Weights\n        :members:\n        :noindex:\n    ", "\n        MobileNet V2 main class\n\n        Args:\n           Inherits args from floating point MobileNetV2\n        ", "https://download.pytorch.org/models/quantized/mobilenet_v2_qnnpack_37f702c5.pth", "backend", "qnnpack", "num_params", "min_size", "categories", "backend", "recipe", "unquantized", "_metrics", "_ops", "_file_size", "_docs", "qnnpack", "https://github.com/pytorch/vision/tree/main/references/classification#qat-mobilenetv2", "\n                These weights were produced by doing Quantization Aware Training (eager mode) on top of the unquantized\n                weights listed below.\n            ", "num_classes", "backend", "ImageNet-1K", "url", "transforms", "meta", "backend", "crop_size", "acc@1", "acc@5", "categories", "backend", "0", "1", "2", "inplace", "inplace"]}