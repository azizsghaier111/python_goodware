{"imports": ["backend", "layers", "keras_export", "imagenet_utils", "Functional", "operation_utils", "file_utils"], "function_calls": ["format", "obtain_input_shape", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "concatenate", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "concatenate", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "concatenate", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "concatenate", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "concatenate", "range", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "concatenate", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "concatenate", "range", "Functional", "keras_export", "preprocess_input", "keras_export", "decode_predictions", "keras_export", "exists", "ValueError", "ValueError", "image_data_format", "Input", "image_data_format", "MaxPooling2D", "MaxPooling2D", "AveragePooling2D", "AveragePooling2D", "AveragePooling2D", "MaxPooling2D", "AveragePooling2D", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "concatenate", "AveragePooling2D", "MaxPooling2D", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "concatenate", "conv2d_bn", "conv2d_bn", "conv2d_bn", "conv2d_bn", "concatenate", "conv2d_bn", "concatenate", "validate_activation", "get_source_inputs", "load_weights", "load_weights", "image_data_format", "Conv2D", "BatchNormalization", "Activation", "is_keras_tensor", "Input", "AveragePooling2D", "AveragePooling2D", "GlobalAveragePooling2D", "Dense", "get_file", "get_file", "frozenset", "str", "str", "str", "GlobalAveragePooling2D", "GlobalMaxPooling2D"], "strings": ["https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5", "https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5", "Instantiates the Inception v3 architecture.\n\n    Reference:\n    - [Rethinking the Inception Architecture for Computer Vision](\n        http://arxiv.org/abs/1512.00567) (CVPR 2016)\n\n    This function returns a Keras image classification model,\n    optionally loaded with weights pre-trained on ImageNet.\n\n    For image classification use cases, see\n    [this page for detailed examples](\n      https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n\n    For transfer learning use cases, make sure to read the\n    [guide to transfer learning & fine-tuning](\n      https://keras.io/guides/transfer_learning/).\n\n    Note: each Keras Application expects a specific kind of input preprocessing.\n    For `InceptionV3`, call\n    `keras.applications.inception_v3.preprocess_input` on your inputs\n    before passing them to the model.\n    `inception_v3.preprocess_input` will scale input pixels between -1 and 1.\n\n    Args:\n        include_top: Boolean, whether to include the fully-connected\n            layer at the top, as the last layer of the network.\n            Defaults to `True`.\n        weights: One of `None` (random initialization),\n            `imagenet` (pre-training on ImageNet),\n            or the path to the weights file to be loaded.\n            Defaults to `\"imagenet\"`.\n        input_tensor: Optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model. `input_tensor` is useful for\n            sharing inputs between multiple different networks.\n            Defaults to `None`.\n        input_shape: Optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(299, 299, 3)` (with `channels_last` data format)\n            or `(3, 299, 299)` (with `channels_first` data format).\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 75.\n            E.g. `(150, 150, 3)` would be one valid value.\n            `input_shape` will be ignored if the `input_tensor` is provided.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` (default) means that the output of the model will be\n                the 4D tensor output of the last convolutional block.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional block, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is `True`, and\n            if no `weights` argument is specified. Defaults to 1000.\n        classifier_activation: A `str` or callable. The activation function\n            to use on the \"top\" layer. Ignored unless `include_top=True`.\n            Set `classifier_activation=None` to return the logits of the \"top\"\n            layer. When loading pretrained weights, `classifier_activation`\n            can only be `None` or `\"softmax\"`.\n        name: The name of the model (string).\n\n    Returns:\n        A model instance.\n    ", "Utility function to apply conv + BN.\n\n    Args:\n        x: input tensor.\n        filters: filters in `Conv2D`.\n        num_row: height of the convolution kernel.\n        num_col: width of the convolution kernel.\n        padding: padding mode in `Conv2D`.\n        strides: strides in `Conv2D`.\n        name: name of the ops; will become `name + '_conv'`\n            for the convolution and `name + '_bn'` for the\n            batch norm layer.\n\n    Returns:\n        Output tensor after applying `Conv2D` and `BatchNormalization`.\n    ", "", "imagenet", "softmax", "inception_v3", "channels_first", "valid", "valid", "valid", "valid", "mixed0", "mixed1", "mixed2", "valid", "valid", "mixed3", "mixed4", "mixed7", "valid", "valid", "mixed8", "imagenet", "same", "channels_first", "tf", "keras.applications.inception_v3.preprocess_input", "keras.applications.inception_v3.decode_predictions", "imagenet", "same", "same", "same", "same", "same", "avg", "keras.applications.inception_v3.InceptionV3", "keras.applications.InceptionV3", "_bn", "_conv", "relu", "mode", "ret", "error", "The `weights` argument should be either `None` (random initialization), `imagenet` (pre-training on ImageNet), or the path to the weights file to be loaded; Received: weights=", "If using `weights=\"imagenet\"` with `include_top=True`, `classes` should be 1000. Received classes=", "default_size", "min_size", "data_format", "require_flatten", "weights", "strides", "padding", "padding", "padding", "padding", "axis", "name", "axis", "name", "axis", "name", "strides", "padding", "strides", "padding", "axis", "name", "axis", "name", "same", "mixed", "axis", "name", "strides", "padding", "strides", "padding", "axis", "name", "mixed9_", "same", "mixed", "avg_pool", "predictions", "max", "name", "inception_v3_weights_tf_dim_ordering_tf_kernels.h5", "models", "9a0d58056eeedaa3f26cb7ebd46da564", "inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5", "models", "bcbd6486424b2319ff4ef7d526e38f63", "data_format", "mode", "top", "shape", "strides", "strides", "strides", "padding", "strides", "padding", "strides", "padding", "strides", "strides", "padding", "axis", "name", "strides", "padding", "strides", "axis", "name", "axis", "axis", "name", "strides", "padding", "use_bias", "name", "axis", "scale", "name", "name", "imagenet", "tensor", "shape", "strides", "padding", "strides", "padding", "name", "activation", "name", "cache_subdir", "file_hash", "cache_subdir", "file_hash"]}