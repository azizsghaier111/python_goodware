{"imports": ["partial", "Any", "List", "Optional", "Union", "torch", "nn", "Tensor", "DeQuantStub", "QuantStub", "Conv2dNormActivation", "SqueezeExcitation", "ImageClassification", "register_model", "Weights", "WeightsEnum", "_IMAGENET_CATEGORIES", "_ovewrite_named_param", "handle_legacy_interface", "_mobilenet_v3_conf", "InvertedResidual", "InvertedResidualConfig", "MobileNet_V3_Large_Weights", "MobileNetV3", "_fuse_modules", "_replace_relu"], "function_calls": ["pop", "Weights", "mul", "_fuse_modules", "get", "hasattr", "_load_from_state_dict", "block", "quant", "_forward_impl", "dequant", "modules", "_ovewrite_named_param", "partial", "verify", "_scale", "add", "len", "_ovewrite_named_param", "items", "super", "block", "type", "_fuse_modules", "type", "fuse_model", "tensor", "tensor", "tensor", "tensor", "tensor", "tensor", "append", "len", "type"], "strings": ["QuantizableMobileNetV3", "MobileNet_V3_Large_QuantizedWeights", "mobilenet_v3_large", "\n    MobileNetV3 (Large) model from\n    `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`_.\n\n    .. note::\n        Note that ``quantize = True`` returns a quantized model with 8 bit\n        weights. Quantized models only support inference and run on CPUs.\n        GPU inference is not yet supported.\n\n    Args:\n        weights (:class:`~torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights` or :class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The\n            pretrained weights for the model. See\n            :class:`~torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        quantize (bool): If True, return a quantized version of the model. Default is False.\n        **kwargs: parameters passed to the ``torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/mobilenetv3.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights\n        :members:\n    .. autoclass:: torchvision.models.MobileNet_V3_Large_Weights\n        :members:\n        :noindex:\n    ", "\n        MobileNet V3 main class\n\n        Args:\n           Inherits args from floating point MobileNetV3\n        ", "inverted_residual_setting", "last_channel", "weights", "progress", "quantize", "kwargs", "return", "backend", "qnnpack", "https://download.pytorch.org/models/quantized/mobilenet_v3_large_qnnpack-5bcacf28.pth", "scale_activation", "version", "qconfig", "num_classes", "backend", "num_params", "min_size", "categories", "backend", "recipe", "unquantized", "_metrics", "_ops", "_file_size", "_docs", "qnnpack", "https://github.com/pytorch/vision/tree/main/references/classification#qat-mobilenetv3", "\n                These weights were produced by doing Quantization Aware Training (eager mode) on top of the unquantized\n                weights listed below.\n            ", "fc1", "activation", "backend", "ImageNet-1K", "url", "transforms", "meta", "inplace", "scale_activation.activation_post_process.scale", "scale_activation.activation_post_process.activation_post_process.scale", "scale_activation.activation_post_process.zero_point", "scale_activation.activation_post_process.activation_post_process.zero_point", "scale_activation.activation_post_process.fake_quant_enabled", "scale_activation.activation_post_process.observer_enabled", "0", "1", "categories", "backend", "crop_size", "acc@1", "acc@5", "2", "inplace", "dtype", "dtype"]}